//! Vector store trait and implementations for semantic caching and RAG.
//!
//! This module provides the `VectorStore` trait which abstracts vector database
//! operations needed for semantic similarity search in response caching and
//! RAG (Retrieval-Augmented Generation) collections.
//!
//! # Implementations
//!
//! - [`PgvectorStore`]: PostgreSQL with pgvector extension
//! - [`QdrantStore`]: Qdrant vector database via HTTP API
//! - [`TestVectorStore`]: No-op implementation for testing
//!
//! # Usage
//!
//! The trait supports two main use cases:
//! 1. **Semantic caching**: Store request embeddings and retrieve similar cached responses
//! 2. **vector stores**: Store document chunks with embeddings for retrieval during chat
//!
//! # Hybrid Search
//!
//! For hybrid search combining vector and keyword results, see the [`fusion`] module
//! which provides Reciprocal Rank Fusion (RRF) for combining ranked result lists.

pub mod fusion;
#[cfg(feature = "database-postgres")]
mod pgvector;
mod qdrant;
#[cfg(test)]
mod test;

#[cfg(test)]
mod tests;

use std::time::Duration;

use async_trait::async_trait;
pub use fusion::{HybridSearchConfig, RrfConfig};
#[cfg(feature = "database-postgres")]
pub use pgvector::PgvectorStore;
pub use qdrant::QdrantStore;
use serde::{Deserialize, Serialize};
// Re-export only when database features are available, since consumers
// (route-level integration tests) require a database to construct the app context.
#[cfg(all(test, any(feature = "database-sqlite", feature = "database-postgres")))]
pub use test::{MockableTestVectorStore, TestVectorStore};
use thiserror::Error;
use uuid::Uuid;

use crate::models::AttributeFilter;

/// Errors that can occur during vector store operations.
#[derive(Debug, Error)]
pub enum VectorStoreError {
    /// Database connection or query error.
    #[error("Database error: {0}")]
    Database(String),

    /// Vector dimension mismatch.
    #[error("Dimension mismatch: expected {expected}, got {actual}")]
    DimensionMismatch { expected: usize, actual: usize },

    /// Serialization error.
    #[error("Serialization error: {0}")]
    Serialization(String),

    /// HTTP client error (for external backends like Qdrant).
    #[error("HTTP error: {0}")]
    Http(String),

    /// Configuration error.
    #[error("Configuration error: {0}")]
    Config(String),

    /// Backend not available.
    #[error("Backend not available: {0}")]
    Unavailable(String),
}

/// Result type for vector store operations.
pub type VectorStoreResult<T> = Result<T, VectorStoreError>;

/// Metadata stored alongside vector embeddings.
///
/// This contains the information needed to retrieve the cached response
/// and validate that the semantic match is appropriate.
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct VectorMetadata {
    /// The exact cache key (SHA-256 hash) for the response.
    /// Used to retrieve the actual cached response from the main cache.
    pub cache_key: String,

    /// The model used for the original request.
    /// Used to filter results to only match same-model requests.
    pub model: String,

    /// Optional organization ID for multi-tenant isolation.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub organization_id: Option<String>,

    /// Optional project ID for finer-grained isolation.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub project_id: Option<String>,

    /// Unix timestamp when the embedding was stored.
    pub created_at: i64,

    /// TTL in seconds (for cleanup of expired entries).
    pub ttl_secs: u64,
}

/// A search result from the vector store.
#[derive(Debug, Clone)]
pub struct VectorSearchResult {
    /// The metadata associated with the matched embedding.
    pub metadata: VectorMetadata,

    /// Cosine similarity score (0.0 to 1.0, higher is more similar).
    pub similarity: f64,
}

// ============================================================================
// RAG VectorStore Chunk Types
// ============================================================================

/// A document chunk with its embedding for storage in the vector store.
///
/// This represents a segment of a document that has been processed for RAG.
/// The embedding is generated by the document processor and stored alongside
/// the chunk content for retrieval.
#[derive(Debug, Clone)]
pub struct ChunkWithEmbedding {
    /// Unique identifier for this chunk
    pub id: Uuid,
    /// The vector store this chunk belongs to
    pub vector_store_id: Uuid,
    /// The file this chunk was extracted from
    pub file_id: Uuid,
    /// Sequential index within the file (0-based)
    pub chunk_index: i32,
    /// The actual text content of the chunk
    pub content: String,
    /// Number of tokens in this chunk
    pub token_count: i32,
    /// Character offset where this chunk starts in the original file
    pub char_start: i32,
    /// Character offset where this chunk ends in the original file
    pub char_end: i32,
    /// The embedding vector for this chunk
    pub embedding: Vec<f64>,
    /// Optional additional metadata (e.g., section headers, attributes)
    pub metadata: Option<serde_json::Value>,
    /// Processing version for atomic shadow-copy updates.
    /// All chunks from a single processing run share the same version.
    /// After successful processing, old version chunks are deleted atomically.
    pub processing_version: Uuid,
}

/// A stored chunk as retrieved from the vector store.
///
/// This contains all the information about a chunk without its embedding,
/// which is typically not needed after retrieval.
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct StoredChunk {
    /// Unique identifier for this chunk
    pub id: Uuid,
    /// The vector store this chunk belongs to
    pub vector_store_id: Uuid,
    /// The file this chunk was extracted from
    pub file_id: Uuid,
    /// Sequential index within the file (0-based)
    pub chunk_index: i32,
    /// The actual text content of the chunk
    pub content: String,
    /// Number of tokens in this chunk
    pub token_count: i32,
    /// Character offset where this chunk starts in the original file
    pub char_start: i32,
    /// Character offset where this chunk ends in the original file
    pub char_end: i32,
    /// Optional additional metadata
    pub metadata: Option<serde_json::Value>,
    /// Unix timestamp when the chunk was created
    pub created_at: i64,
    /// Processing version that created this chunk
    pub processing_version: Uuid,
}

/// A search result from a vector store chunk search.
#[derive(Debug, Clone)]
pub struct ChunkSearchResult {
    /// The chunk ID
    pub chunk_id: Uuid,
    /// The vector store this chunk belongs to (useful for multi-vector store searches)
    pub vector_store_id: Uuid,
    /// The file this chunk was extracted from
    pub file_id: Uuid,
    /// Sequential index within the file
    pub chunk_index: i32,
    /// The actual text content of the chunk
    pub content: String,
    /// Similarity score (0.0 to 1.0, higher is more similar)
    pub score: f64,
    /// Optional additional metadata
    pub metadata: Option<serde_json::Value>,
}

/// Filter options for chunk searches.
#[derive(Debug, Clone, Default)]
pub struct ChunkFilter {
    /// Only search within these specific files
    pub file_ids: Option<Vec<Uuid>>,
    /// Attribute-based filter using OpenAI-compatible schema.
    ///
    /// Supports comparison operators (eq, ne, gt, gte, lt, lte) and
    /// logical operators (and, or) for filtering based on file attributes.
    pub attribute_filter: Option<AttributeFilter>,
}

/// Trait for vector database operations required by semantic caching.
///
/// Implementations of this trait provide the ability to store, search,
/// and delete vector embeddings for semantic similarity matching.
///
/// # Thread Safety
///
/// Implementations must be thread-safe (`Send + Sync`) as they will be
/// shared across async tasks.
///
/// # Example
///
/// ```ignore
/// use crate::cache::vector_store::{VectorBackend, VectorMetadata};
///
/// async fn example(store: &dyn VectorBackend) {
///     // Store an embedding
///     let embedding = vec![0.1, 0.2, 0.3, /* ... */];
///     let metadata = VectorMetadata {
///         cache_key: "sha256:abc123...".to_string(),
///         model: "gpt-4".to_string(),
///         organization_id: None,
///         project_id: None,
///         created_at: 1699999999,
///         ttl_secs: 3600,
///     };
///     store.store("unique-id", &embedding, metadata, Duration::from_secs(3600)).await?;
///
///     // Search for similar embeddings
///     let query = vec![0.11, 0.19, 0.31, /* ... */];
///     let results = store.search(&query, 5, 0.95, Some("gpt-4")).await?;
///
///     // Delete an embedding
///     store.delete("unique-id").await?;
/// }
/// ```
#[async_trait]
pub trait VectorBackend: Send + Sync {
    /// Store an embedding with associated metadata.
    ///
    /// # Arguments
    ///
    /// * `id` - Unique identifier for this embedding (typically the cache key)
    /// * `embedding` - The vector embedding (must match configured dimensions)
    /// * `metadata` - Metadata to store with the embedding
    /// * `ttl` - Time-to-live for the entry (implementations may handle expiry differently)
    ///
    /// # Returns
    ///
    /// `Ok(())` on success, or an error if storage fails.
    async fn store(
        &self,
        id: &str,
        embedding: &[f64],
        metadata: VectorMetadata,
        ttl: Duration,
    ) -> VectorStoreResult<()>;

    /// Search for similar embeddings.
    ///
    /// # Arguments
    ///
    /// * `embedding` - The query embedding to find similar vectors for
    /// * `limit` - Maximum number of results to return
    /// * `threshold` - Minimum similarity threshold (0.0 to 1.0)
    /// * `model_filter` - Optional model name to filter results (only return same-model matches)
    ///
    /// # Returns
    ///
    /// A vector of search results sorted by similarity (highest first),
    /// filtered to only include results above the threshold.
    async fn search(
        &self,
        embedding: &[f64],
        limit: usize,
        threshold: f64,
        model_filter: Option<&str>,
    ) -> VectorStoreResult<Vec<VectorSearchResult>>;

    /// Delete an embedding by its ID.
    ///
    /// # Arguments
    ///
    /// * `id` - The unique identifier of the embedding to delete
    ///
    /// # Returns
    ///
    /// `Ok(())` on success (including if the entry didn't exist),
    /// or an error if deletion fails.
    async fn delete(&self, id: &str) -> VectorStoreResult<()>;

    /// Delete all expired embeddings.
    ///
    /// This should be called periodically to clean up entries whose TTL has passed.
    /// Some backends may handle this automatically.
    ///
    /// # Returns
    ///
    /// The number of deleted entries, or an error if cleanup fails.
    async fn cleanup_expired(&self) -> VectorStoreResult<usize>;

    /// Get the configured embedding dimensions.
    fn dimensions(&self) -> usize;

    /// Check if the backend is healthy and available.
    async fn health_check(&self) -> VectorStoreResult<()>;

    // ========================================================================
    // RAG VectorStore Chunk Operations
    // ========================================================================

    /// Store multiple document chunks with their embeddings.
    ///
    /// This is the primary method for indexing document chunks for RAG.
    /// Chunks are stored with their full content and metadata for retrieval.
    ///
    /// # Arguments
    ///
    /// * `chunks` - The chunks with embeddings to store
    ///
    /// # Returns
    ///
    /// `Ok(())` on success, or an error if storage fails.
    /// Uses upsert semantics - existing chunks with the same ID are replaced.
    async fn store_chunks(&self, chunks: Vec<ChunkWithEmbedding>) -> VectorStoreResult<()>;

    /// Retrieve all chunks for a specific file.
    ///
    /// Returns chunks ordered by chunk_index for sequential reading.
    ///
    /// # Arguments
    ///
    /// * `file_id` - The file ID to retrieve chunks for
    ///
    /// # Returns
    ///
    /// All chunks for the file, ordered by chunk_index.
    async fn get_chunks_by_file(&self, file_id: Uuid) -> VectorStoreResult<Vec<StoredChunk>>;

    /// Delete all chunks for a specific file.
    ///
    /// **Warning:** This deletes chunks across ALL vector stores. If the same file
    /// exists in multiple vector stores, use [`delete_chunks_by_file_and_vector_store`]
    /// instead to avoid affecting other vector stores.
    ///
    /// # Arguments
    ///
    /// * `file_id` - The file ID whose chunks should be deleted
    ///
    /// # Returns
    ///
    /// The number of chunks deleted.
    async fn delete_chunks_by_file(&self, file_id: Uuid) -> VectorStoreResult<u64>;

    /// Delete chunks for a specific file within a specific vector_store.
    ///
    /// This is the preferred method when removing a file from a vector_store,
    /// as it only affects chunks in the target vector store and leaves chunks
    /// for the same file in other vector stores untouched.
    ///
    /// # Arguments
    ///
    /// * `file_id` - The file ID whose chunks should be deleted
    /// * `vector_store_id` - The vector store to delete chunks from
    ///
    /// # Returns
    ///
    /// The number of chunks deleted.
    async fn delete_chunks_by_file_and_vector_store(
        &self,
        file_id: Uuid,
        vector_store_id: Uuid,
    ) -> VectorStoreResult<u64>;

    /// Delete chunks for a file in a vector_store, except those with a specific version.
    ///
    /// This method is used for atomic shadow-copy updates during document processing.
    /// The pattern is:
    /// 1. Generate a new `processing_version` UUID for this processing run
    /// 2. Store all new chunks with that version
    /// 3. Call this method to delete old chunks (those with different versions)
    ///
    /// This ensures atomicity: if chunk storage fails partway through, old chunks
    /// remain intact. Only after all new chunks are successfully stored are the
    /// old chunks removed.
    ///
    /// # Arguments
    ///
    /// * `file_id` - The file ID whose old chunks should be deleted
    /// * `vector_store_id` - The vector store to delete chunks from
    /// * `keep_version` - The processing version to keep (all other versions are deleted)
    ///
    /// # Returns
    ///
    /// The number of chunks deleted.
    async fn delete_chunks_by_file_and_vector_store_except_version(
        &self,
        file_id: Uuid,
        vector_store_id: Uuid,
        keep_version: Uuid,
    ) -> VectorStoreResult<u64>;

    /// Delete all chunks for a specific vector_store.
    ///
    /// Used when a vector store is deleted.
    ///
    /// # Arguments
    ///
    /// * `vector_store_id` - The vector store ID whose chunks should be deleted
    ///
    /// # Returns
    ///
    /// The number of chunks deleted.
    async fn delete_chunks_by_vector_store(&self, vector_store_id: Uuid) -> VectorStoreResult<u64>;

    /// Search for similar chunks within a single vector_store.
    ///
    /// # Arguments
    ///
    /// * `vector_store_id` - The vector store to search within
    /// * `embedding` - The query embedding
    /// * `limit` - Maximum number of results
    /// * `threshold` - Minimum similarity threshold (0.0 to 1.0)
    /// * `filter` - Optional additional filters
    ///
    /// # Returns
    ///
    /// Matching chunks sorted by similarity (highest first).
    async fn search_vector_store(
        &self,
        vector_store_id: Uuid,
        embedding: &[f64],
        limit: usize,
        threshold: f64,
        filter: Option<ChunkFilter>,
    ) -> VectorStoreResult<Vec<ChunkSearchResult>>;

    /// Search for similar chunks across multiple vector stores.
    ///
    /// This is a Hadrian extension for searching across multiple vector stores
    /// in a single query. All vector stores must have the same embedding model
    /// and dimensions (validated by the caller).
    ///
    /// # Arguments
    ///
    /// * `vector_store_ids` - The collections to search across
    /// * `embedding` - The query embedding
    /// * `limit` - Maximum number of results (total across all vector stores)
    /// * `threshold` - Minimum similarity threshold (0.0 to 1.0)
    /// * `filter` - Optional additional filters
    ///
    /// # Returns
    ///
    /// Matching chunks from all vector stores, sorted by similarity (highest first).
    async fn search_vector_stores(
        &self,
        vector_store_ids: &[Uuid],
        embedding: &[f64],
        limit: usize,
        threshold: f64,
        filter: Option<ChunkFilter>,
    ) -> VectorStoreResult<Vec<ChunkSearchResult>>;

    // ========================================================================
    // Keyword Search Operations (for Hybrid Search)
    // ========================================================================

    /// Search for chunks matching keywords within a single vector_store.
    ///
    /// This performs full-text keyword search (BM25/TF-IDF style) rather than
    /// semantic vector search. Used as part of hybrid search to find exact
    /// keyword matches that vector search might miss.
    ///
    /// # Arguments
    ///
    /// * `vector_store_id` - The vector store to search within
    /// * `query` - The keyword search query (will be parsed into search terms)
    /// * `limit` - Maximum number of results
    /// * `filter` - Optional additional filters
    ///
    /// # Returns
    ///
    /// Matching chunks sorted by keyword relevance score (highest first).
    /// Scores are normalized to 0.0-1.0 range for consistency with vector search.
    ///
    /// # Backend Notes
    ///
    /// - **PostgreSQL**: Uses `tsvector`/`tsquery` with `ts_rank()` for ranking
    /// - **Qdrant**: Uses text index with keyword matching (less sophisticated)
    async fn keyword_search_vector_store(
        &self,
        vector_store_id: Uuid,
        query: &str,
        limit: usize,
        filter: Option<ChunkFilter>,
    ) -> VectorStoreResult<Vec<ChunkSearchResult>>;

    /// Search for chunks matching keywords across multiple vector stores.
    ///
    /// This performs full-text keyword search across multiple vector stores.
    /// Used as part of hybrid search to find exact keyword matches.
    ///
    /// # Arguments
    ///
    /// * `vector_store_ids` - The collections to search across
    /// * `query` - The keyword search query (will be parsed into search terms)
    /// * `limit` - Maximum number of results (total across all vector stores)
    /// * `filter` - Optional additional filters
    ///
    /// # Returns
    ///
    /// Matching chunks from all vector stores, sorted by keyword relevance (highest first).
    /// Scores are normalized to 0.0-1.0 range for consistency with vector search.
    async fn keyword_search_vector_stores(
        &self,
        vector_store_ids: &[Uuid],
        query: &str,
        limit: usize,
        filter: Option<ChunkFilter>,
    ) -> VectorStoreResult<Vec<ChunkSearchResult>>;

    // ========================================================================
    // Hybrid Search Operations
    // ========================================================================

    /// Perform hybrid search combining vector and keyword search within a single vector_store.
    ///
    /// This runs both semantic (vector) and lexical (keyword) searches in parallel,
    /// then combines results using Reciprocal Rank Fusion (RRF). Hybrid search is
    /// particularly effective for queries that may benefit from both semantic
    /// understanding and exact keyword matching.
    ///
    /// # Arguments
    ///
    /// * `vector_store_id` - The vector store to search within
    /// * `query` - The search query text (used for both vector embedding and keyword search)
    /// * `embedding` - The query embedding for vector search
    /// * `limit` - Maximum number of results
    /// * `config` - Hybrid search configuration (RRF settings, thresholds)
    /// * `filter` - Optional additional filters
    ///
    /// # Returns
    ///
    /// Combined results sorted by RRF score (highest first).
    /// Note: RRF scores are relative rankings, not similarity scores.
    ///
    /// # Example
    ///
    /// ```ignore
    /// let config = HybridSearchConfig::default();
    /// let results = store.hybrid_search_vector_store(
    ///     vector_store_id,
    ///     "Part number XJ-900",
    ///     &query_embedding,
    ///     10,
    ///     config,
    ///     None,
    /// ).await?;
    /// ```
    async fn hybrid_search_vector_store(
        &self,
        vector_store_id: Uuid,
        query: &str,
        embedding: &[f64],
        limit: usize,
        config: HybridSearchConfig,
        filter: Option<ChunkFilter>,
    ) -> VectorStoreResult<Vec<ChunkSearchResult>>;

    /// Perform hybrid search combining vector and keyword search across multiple vector stores.
    ///
    /// This runs both semantic (vector) and lexical (keyword) searches in parallel,
    /// then combines results using Reciprocal Rank Fusion (RRF).
    ///
    /// # Arguments
    ///
    /// * `vector_store_ids` - The collections to search across
    /// * `query` - The search query text (used for both vector embedding and keyword search)
    /// * `embedding` - The query embedding for vector search
    /// * `limit` - Maximum number of results (total across all vector stores)
    /// * `config` - Hybrid search configuration (RRF settings, thresholds)
    /// * `filter` - Optional additional filters
    ///
    /// # Returns
    ///
    /// Combined results from all vector stores, sorted by RRF score (highest first).
    async fn hybrid_search_vector_stores(
        &self,
        vector_store_ids: &[Uuid],
        query: &str,
        embedding: &[f64],
        limit: usize,
        config: HybridSearchConfig,
        filter: Option<ChunkFilter>,
    ) -> VectorStoreResult<Vec<ChunkSearchResult>>;
}

#[cfg(test)]
mod unit_tests {
    use super::*;

    #[test]
    fn test_vector_metadata_serialization() {
        let metadata = VectorMetadata {
            cache_key: "sha256:abc123".to_string(),
            model: "gpt-4".to_string(),
            organization_id: Some("org-123".to_string()),
            project_id: None,
            created_at: 1699999999,
            ttl_secs: 3600,
        };

        let json = serde_json::to_string(&metadata).unwrap();
        let parsed: VectorMetadata = serde_json::from_str(&json).unwrap();

        assert_eq!(parsed.cache_key, "sha256:abc123");
        assert_eq!(parsed.model, "gpt-4");
        assert_eq!(parsed.organization_id, Some("org-123".to_string()));
        assert_eq!(parsed.project_id, None);
        assert_eq!(parsed.created_at, 1699999999);
        assert_eq!(parsed.ttl_secs, 3600);
    }

    #[test]
    fn test_vector_metadata_without_optional_fields() {
        let json = r#"{
            "cache_key": "key123",
            "model": "claude-3",
            "created_at": 1700000000,
            "ttl_secs": 7200
        }"#;

        let metadata: VectorMetadata = serde_json::from_str(json).unwrap();

        assert_eq!(metadata.cache_key, "key123");
        assert_eq!(metadata.model, "claude-3");
        assert_eq!(metadata.organization_id, None);
        assert_eq!(metadata.project_id, None);
        assert_eq!(metadata.created_at, 1700000000);
        assert_eq!(metadata.ttl_secs, 7200);
    }

    #[test]
    fn test_vector_store_error_display() {
        let err = VectorStoreError::DimensionMismatch {
            expected: 1536,
            actual: 768,
        };
        assert_eq!(
            err.to_string(),
            "Dimension mismatch: expected 1536, got 768"
        );

        let err = VectorStoreError::Database("connection failed".to_string());
        assert_eq!(err.to_string(), "Database error: connection failed");
    }
}
